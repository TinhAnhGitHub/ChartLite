general:
  fast_dev_run: False
  seed: 42

model:
  backbone_path: google/matcha-base
  max_length: 512
  max_patches: 1024
  patch_size: 16
  len_tokenizer: ???
  pad_token_id: ???
  decoder_start_token_id: ???
  bos_token_id: ???
  max_length_generation: 512
  frozen_percentage: 0.4


train_params:
  train_bs: 2
  # = 38GB VRAM = 16 * 2 GB VRAM
  val_bs: 2
  # = ??? cung cung 16 + 38 GB VRAM = 54 GB VRAM > 58GB VRAM
  max_steps: 58624  
  grad_accumulation: 4
  use_fp16_mixed: true
  num_workers: 4
  val_check_interval: 5
  save_every_n_train_steps: 500

optimizer:
  lr: 1e-4
  weight_decay: 0.005
  grad_clip_value: 1.0

learning_rate_scheduler:
  warmup_pct: 0.0




metrics_tolerance:
  numeric_tolerance: 0.05
  string_tolerance: 2

outputs:
  model_dir: ./run_expr

best_ckpt:
  monitor: 'val/overall_sim_avg'
  mode: 'max'
  save_top_k: 2

early_stopping:
  monitor: 'val/overall_sim_avg'
  patience: 1000
  mode: 'max'



dataset:
  parquet_dict: D:\Projects\School\Matcha\downsampled_cleaned_chartQa_plotQa_colored\data
  data_ratio: 0.001



wandb:
  enabled: enable
  project: mga-dev-a1
  run_name: experiment-bug


images:
  rsz_height: 512
  rsz_width: 512


vpn:
  nodes: 2
  devices : 2
  name: matcha_t