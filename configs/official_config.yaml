general:
  fast_dev_run: false
  seed: 42

model:
  backbone_path: google/matcha-base
  max_length: 512
  max_patches: 1024
  patch_size: 16
  len_tokenizer: ???
  pad_token_id: ???
  decoder_start_token_id: ???
  bos_token_id: ???
  max_length_generation: 1024
  frozen_percentage: 0.8
  freeze_layernorm: true  
  freeze_pos_emb: true  

train_params:
  train_bs: 1
  val_bs: 1
  max_steps: 1465570  # 1_465_570, a single epoch will have 146_557 train rows
  grad_accumulation: 16
  use_fp16_mixed: true
  num_workers: 20 
  val_check_interval: 30000  # 80_000 
  save_every_n_train_steps: 30000 # 80_000 
  ema_enable: false
  ema_decay: 0.9999
  ema_validate_original_weights: false
  ema_every_n_steps: 1
  ema_cpu_offload: true

  awp_enable: false
  awp_adv_lr: 0.005
  adv_eps: 0.001
  apply_every: 2

optimizer:
  lr: 7e-5
  weight_decay: 0.01
  grad_clip_value: 1.0

learning_rate_scheduler:
  warmup_pct: 0.05
  num_cycles: 10


metrics_tolerance:
  numeric_tolerance: 0.05
  string_tolerance: 2

outputs:
  model_dir: ./run_expr/exp

best_ckpt:
  monitor: 'val/loss_avg'
  mode: 'max'
  save_top_k: 2

early_stopping:
  monitor: 'val/loss_avg'
  patience: 10
  mode: 'max'


# total step for 1 epoch = 14656 step
# 4 epeoch = 58624
#2200  step = 0.15 epoch
#586 step = 0.04 epoch
# 146 p = 2h20
#9h10 chay dc 0.15 epoch  = 2200 step
dataset:
  parquet_dict: ./downsampled_cleaned_chartQa_plotQa_colored/data
  data_ratio: 1



wandb:
  enabled: true
  project: mga-dev-a1
  run_name: full_encoder_training_test


images:
  rsz_height: 512
  rsz_width: 512



vpn:
  nodes: 2
  devices : 2
  name: matcha_t
