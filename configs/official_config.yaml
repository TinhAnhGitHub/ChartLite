general:
  fast_dev_run: False
  seed: 42

model:
  backbone_path: google/matcha-base
  max_length: 512
  max_patches: 1024
  patch_size: 16
  len_tokenizer: ???
  pad_token_id: ???
  decoder_start_token_id: ???
  bos_token_id: ???
  max_length_generation: 512
  frozen_percentage: 0.4


train_params:
  train_bs: 1
  # = 38GB VRAM = 16 * 2 GB VRAM
  val_bs: 1
  # = ??? cung cung 16 + 38 GB VRAM = 54 GB VRAM > 58GB VRAM
  max_steps: 146560 
  grad_accumulation: 8
  use_fp16_mixed: true
  num_workers: 8
  val_check_interval: 1000
  save_every_n_train_steps: 1000

  ema_enable: false
  ema_decay: 0.9999
  ema_validate_original_weights: False # set true to validate with original weights, should set to False
  ema_every_n_steps: 1
  ema_cpu_offload: True # Set to True to store EMA weights on CPU to save GPU memory
  
optimizer:
  lr: 5e-5
  weight_decay: 0.01
  grad_clip_value: 1.0

learning_rate_scheduler:
  warmup_pct: 0.0
  num_cycles: 900




metrics_tolerance:
  numeric_tolerance: 0.05
  string_tolerance: 2

outputs:
  model_dir: ./run_expr

best_ckpt:
  monitor: 'val/overall_sim_avg'
  mode: 'max'
  save_top_k: 2

early_stopping:
  monitor: 'val/overall_sim_avg'
  patience: 1000
  mode: 'max'


# total step for 1 epoch = 14656 step
# 4 epeoch = 58624
#2200  step = 0.15 epoch
#586 step = 0.04 epoch
# 146 p = 2h20
#9h10 chay dc 0.15 epoch  = 2200 step
dataset:
  parquet_dict: ./downsampled_cleaned_chartQa_plotQa_colored/data
  data_ratio: 1



wandb:
  enabled: false
  project: mga-dev-a1
  run_name: experiment-bug


images:
  rsz_height: 512
  rsz_width: 512


# vpn:
#   nodes: 2
#   devices : 2
#   name: matcha_t
